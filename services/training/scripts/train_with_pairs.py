"""
è®­ç»ƒé€‚é…å™¨ï¼šä»å›¾åƒå¯¹æ–‡ä»¶åŠ è½½æ•°æ®å¹¶è®­ç»ƒCLIPæ¨¡å‹
ä¾›main.pyçš„step4è°ƒç”¨

åŠŸèƒ½ï¼š
- ä»NPYæ–‡ä»¶åŠ è½½å·²æ¸²æŸ“çš„å›¾åƒå¯¹ï¼ˆanchorå’Œpositiveï¼‰
- åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
- åˆå§‹åŒ–CLIPæ¨¡å‹å’Œè®­ç»ƒå™¨
- æ‰§è¡Œå¯¹æ¯”å­¦ä¹ è®­ç»ƒ
"""
import sys
from pathlib import Path
from typing import Tuple
import numpy as np
import torch
from torch.utils.data import DataLoader, Dataset
from PIL import Image
import torchvision.transforms as transforms

# æ·»åŠ é¡¹ç›®è·¯å¾„
script_dir = Path(__file__).parent
training_dir = script_dir.parent
project_root = training_dir.parent.parent

sys.path.insert(0, str(project_root))
sys.path.insert(0, str(training_dir))

from clip_contrastive_trainer import (
    CLIPEncoder, 
    CLIPTrainer, 
    ContrastiveLoss,
    DataAugmentation
)


class ImagePairDataset(Dataset):
    """
    ä»NPYæ–‡ä»¶åŠ è½½å›¾åƒå¯¹çš„æ•°æ®é›†
    
    ç”¨äºä»main.pyçš„step3ç”Ÿæˆçš„å›¾åƒå¯¹æ–‡ä»¶åŠ è½½æ•°æ®
    """
    
    def __init__(self, anchor_images_file: str, positive_images_file: str, apply_augmentation: bool = True):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            anchor_images_file: Anchorå›¾åƒNPYæ–‡ä»¶è·¯å¾„
            positive_images_file: Positiveå›¾åƒNPYæ–‡ä»¶è·¯å¾„
            apply_augmentation: æ˜¯å¦åº”ç”¨æ•°æ®å¢å¼º
        """
        print(f"ğŸ“ Loading anchor images from: {anchor_images_file}")
        self.anchor_images = np.load(anchor_images_file)  # [N, H, W, 3]
        
        print(f"ğŸ“ Loading positive images from: {positive_images_file}")
        self.positive_images = np.load(positive_images_file)  # [N, H, W, 3]
        
        if len(self.anchor_images) != len(self.positive_images):
            raise ValueError(
                f"Mismatch: {len(self.anchor_images)} anchor images vs "
                f"{len(self.positive_images)} positive images"
            )
        
        print(f"âœ… Loaded {len(self.anchor_images)} image pairs")
        print(f"   Image shape: {self.anchor_images[0].shape}")
        
        self.apply_augmentation = apply_augmentation
        
        if self.apply_augmentation:
            self.augmenter = DataAugmentation()
            print("âœ… Data augmentation enabled")
        
        # è½¬æ¢ä¸ºtensorçš„transform
        self.to_tensor = transforms.ToTensor()
    
    def __len__(self):
        return len(self.anchor_images)
    
    def __getitem__(self, idx):
        """
        è·å–ä¸€ä¸ªå›¾åƒå¯¹
        
        Returns:
            dict with keys: "anchor", "positive"
            - anchor: tensor [C, H, W]
            - positive: tensor [C, H, W]
        """
        anchor = self.anchor_images[idx]  # [H, W, 3], uint8
        positive = self.positive_images[idx]  # [H, W, 3], uint8
        
        # ç¡®ä¿æ˜¯uint8ç±»å‹
        if anchor.dtype != np.uint8:
            anchor = (anchor * 255).astype(np.uint8) if anchor.max() <= 1.0 else anchor.astype(np.uint8)
        if positive.dtype != np.uint8:
            positive = (positive * 255).astype(np.uint8) if positive.max() <= 1.0 else positive.astype(np.uint8)
        
        # è½¬æ¢ä¸ºPIL Image
        anchor_pil = Image.fromarray(anchor)
        positive_pil = Image.fromarray(positive)
        
        # æ•°æ®å¢å¼ºï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.apply_augmentation:
            anchor_pil = self.augmenter.augment_image(anchor_pil)
            positive_pil = self.augmenter.augment_image(positive_pil)
        
        # è½¬æ¢ä¸ºtensor [C, H, W]ï¼Œå€¼åŸŸ[0, 1]
        anchor_tensor = self.to_tensor(anchor_pil)
        positive_tensor = self.to_tensor(positive_pil)
        
        return {
            "anchor": anchor_tensor,
            "positive": positive_tensor
        }


def train_with_pairs(
    anchor_images_file: str,
    positive_images_file: str,
    pairs_metadata_file: str,
    num_epochs: int = 50,
    batch_size: int = 16,
    learning_rate: float = 5e-5,
    weight_decay: float = 0.01,
    model_name: str = "ViT-B/32",
    embedding_dim: int = 512,
    image_size: int = 224,
    apply_augmentation: bool = True,
    log_dir: str = "services/training/logs/clip_contrastive",
    device: str = "auto",
    val_split: float = 0.2
) -> Tuple[CLIPTrainer, CLIPEncoder]:
    """
    ä»å›¾åƒå¯¹æ–‡ä»¶è®­ç»ƒCLIPæ¨¡å‹
    
    è¿™æ˜¯main.pyçš„step4è°ƒç”¨çš„è®­ç»ƒå‡½æ•°ï¼Œä»å·²å‡†å¤‡å¥½çš„å›¾åƒå¯¹æ–‡ä»¶åŠ è½½æ•°æ®å¹¶è®­ç»ƒã€‚
    
    Args:
        anchor_images_file: Anchorå›¾åƒNPYæ–‡ä»¶è·¯å¾„
        positive_images_file: Positiveå›¾åƒNPYæ–‡ä»¶è·¯å¾„
        pairs_metadata_file: ç›¸ä¼¼å¯¹å…ƒæ•°æ®JSONæ–‡ä»¶è·¯å¾„ï¼ˆå½“å‰æœªä½¿ç”¨ï¼Œä¿ç•™ç”¨äºæœªæ¥æ‰©å±•ï¼‰
        num_epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹æ¬¡å¤§å°
        learning_rate: å­¦ä¹ ç‡
        weight_decay: æƒé‡è¡°å‡
        model_name: CLIPæ¨¡å‹åç§°
        embedding_dim: åµŒå…¥ç»´åº¦
        image_size: å›¾åƒå°ºå¯¸ï¼ˆå½“å‰æœªä½¿ç”¨ï¼Œä»NPYæ–‡ä»¶è¯»å–ï¼‰
        apply_augmentation: æ˜¯å¦åº”ç”¨æ•°æ®å¢å¼º
        log_dir: æ—¥å¿—ç›®å½•
        device: è®¾å¤‡ç±»å‹ï¼ˆ"auto", "cpu", "cuda"ï¼‰
        val_split: éªŒè¯é›†æ¯”ä¾‹
    
    Returns:
        trainer, model: è®­ç»ƒå™¨å’Œæ¨¡å‹å®ä¾‹
    """
    # è®¾ç½®è®¾å¤‡
    if device == "auto":
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    else:
        device = torch.device(device)
    
    print(f"ğŸ”§ Using device: {device}")
    
    # åˆ›å»ºæ•°æ®é›†
    print("\nğŸ“ Loading image pair datasets...")
    full_dataset = ImagePairDataset(
        anchor_images_file=anchor_images_file,
        positive_images_file=positive_images_file,
        apply_augmentation=apply_augmentation
    )
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    total_size = len(full_dataset)
    train_size = int((1 - val_split) * total_size)
    val_size = total_size - train_size
    
    print(f"\nğŸ“Š Dataset split:")
    print(f"   Total: {total_size} pairs")
    print(f"   Train: {train_size} pairs ({100*(1-val_split):.1f}%)")
    print(f"   Val: {val_size} pairs ({100*val_split:.1f}%)")
    
    train_dataset, val_dataset = torch.utils.data.random_split(
        full_dataset, [train_size, val_size],
        generator=torch.Generator().manual_seed(42)  # å›ºå®šéšæœºç§å­ï¼Œç¡®ä¿å¯å¤ç°
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=2 if torch.cuda.is_available() else 0,  # Windowsä¸Šnum_workers=0æ›´ç¨³å®š
        pin_memory=True if torch.cuda.is_available() else False
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=2 if torch.cuda.is_available() else 0,
        pin_memory=True if torch.cuda.is_available() else False
    )
    
    # åˆ›å»ºæ¨¡å‹
    print(f"\nğŸ—ï¸ Creating CLIP-based model...")
    print(f"   Model: {model_name}")
    print(f"   Embedding dim: {embedding_dim}")
    
    model = CLIPEncoder(
        model_name=model_name,
        embedding_dim=embedding_dim
    )
    
    # åˆ›å»ºæŸå¤±å‡½æ•°
    loss_fn = ContrastiveLoss(temperature=0.07)
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=learning_rate,
        weight_decay=weight_decay
    )
    
    print(f"\nâš™ï¸ Training configuration:")
    print(f"   Batch size: {batch_size}")
    print(f"   Learning rate: {learning_rate}")
    print(f"   Weight decay: {weight_decay}")
    print(f"   Epochs: {num_epochs}")
    print(f"   Log dir: {log_dir}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = CLIPTrainer(
        model=model,
        loss_fn=loss_fn,
        optimizer=optimizer,
        device=device,
        log_dir=log_dir
    )
    
    # å°è¯•ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
    checkpoint_path = Path(log_dir) / "checkpoint_latest.pth"
    if checkpoint_path.exists():
        print(f"\nğŸ“ Found checkpoint: {checkpoint_path}")
        try:
            trainer.load_checkpoint(str(checkpoint_path))
            print("âœ… Resumed training from checkpoint")
        except Exception as e:
            print(f"âš ï¸  Failed to load checkpoint: {e}")
            print("   Starting training from scratch")
    
    # å¼€å§‹è®­ç»ƒ
    print(f"\nğŸš€ Starting training...")
    trainer.train(
        train_loader=train_loader,
        val_loader=val_loader,
        num_epochs=num_epochs
    )
    
    print(f"\nâœ… Training completed!")
    print(f"   Best model: {Path(log_dir) / 'checkpoint_best.pth'}")
    
    return trainer, model


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    import argparse
    
    parser = argparse.ArgumentParser(description="Train CLIP model from image pairs")
    parser.add_argument("--anchor_images", type=str, required=True)
    parser.add_argument("--positive_images", type=str, required=True)
    parser.add_argument("--pairs_metadata", type=str, required=True)
    parser.add_argument("--num_epochs", type=int, default=50)
    parser.add_argument("--batch_size", type=int, default=16)
    parser.add_argument("--learning_rate", type=float, default=5e-5)
    
    args = parser.parse_args()
    
    trainer, model = train_with_pairs(
        anchor_images_file=args.anchor_images,
        positive_images_file=args.positive_images,
        pairs_metadata_file=args.pairs_metadata,
        num_epochs=args.num_epochs,
        batch_size=args.batch_size,
        learning_rate=args.learning_rate
    )
