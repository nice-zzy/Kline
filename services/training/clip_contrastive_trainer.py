"""
åŸºäºCLIPçš„Kçº¿å›¾å¯¹æ¯”å­¦ä¹ è®­ç»ƒå™¨
ä½¿ç”¨2012-2016å¹´æ•°æ®è®­ç»ƒï¼Œ2017å¹´æ•°æ®æµ‹è¯•
5å¤©çª—å£ï¼Œæ­¥é•¿3ï¼Œæ„é€ æ­£æ ·æœ¬å¯¹è¿›è¡Œå¯¹æ¯”å­¦ä¹ 
"""
import os
import sys
import json
import time
import argparse
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings("ignore")

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torch.utils.tensorboard import SummaryWriter
import numpy as np
import pandas as pd
from tqdm import tqdm
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from PIL import Image, ImageEnhance, ImageOps
import torchvision.transforms as transforms

# å°è¯•å¯¼å…¥CLIP
try:
    import clip
    CLIP_AVAILABLE = True
except ImportError:
    CLIP_AVAILABLE = False
    print("âš ï¸ CLIP not available. Install with: pip install clip-openai   (or: pip install git+https://github.com/openai/CLIP.git)")


class CandlestickRenderer:
    """èœ¡çƒ›å›¾æ¸²æŸ“å™¨"""
    
    def __init__(self, image_size: int = 224, dpi: int = 100):
        self.image_size = image_size
        self.dpi = dpi
        self.fig_size = image_size / dpi
    
    def render_candlestick(self, ohlc_data: pd.DataFrame) -> np.ndarray:
        """
        æ¸²æŸ“èœ¡çƒ›å›¾
        
        Args:
            ohlc_data: DataFrame with columns ['open', 'high', 'low', 'close', 'volume']
        
        Returns:
            RGBå›¾åƒæ•°ç»„ [H, W, 3]
        """
        # åˆ›å»ºå›¾å½¢
        fig, ax = plt.subplots(figsize=(self.fig_size, self.fig_size), dpi=self.dpi)
        ax.set_xlim(0, len(ohlc_data))
        ax.set_ylim(ohlc_data['low'].min() * 0.98, ohlc_data['high'].max() * 1.02)
        
        # è®¾ç½®èƒŒæ™¯ä¸ºç™½è‰²
        ax.set_facecolor('white')
        fig.patch.set_facecolor('white')
        
        # ç»˜åˆ¶èœ¡çƒ›å›¾
        for i, (_, row) in enumerate(ohlc_data.iterrows()):
            open_price = row['open']
            high_price = row['high']
            low_price = row['low']
            close_price = row['close']
            
            # ç¡®å®šé¢œè‰²
            color = 'red' if close_price < open_price else 'green'
            
            # ç»˜åˆ¶å½±çº¿
            ax.plot([i, i], [low_price, high_price], color='black', linewidth=1)
            
            # ç»˜åˆ¶å®ä½“
            body_height = abs(close_price - open_price)
            body_bottom = min(open_price, close_price)
            
            if body_height > 0:
                rect = patches.Rectangle(
                    (i - 0.3, body_bottom), 0.6, body_height,
                    facecolor=color, edgecolor='black', linewidth=0.5
                )
                ax.add_patch(rect)
            else:
                # åå­—æ˜Ÿ
                ax.plot([i - 0.3, i + 0.3], [open_price, open_price], color='black', linewidth=2)
        
        # ç§»é™¤åæ ‡è½´
        ax.set_xticks([])
        ax.set_yticks([])
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        ax.spines['bottom'].set_visible(False)
        ax.spines['left'].set_visible(False)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        fig.canvas.draw()
        
        # å…¼å®¹ä¸åŒmatplotlibåç«¯çš„API
        try:
            # æ–°ç‰ˆæœ¬matplotlib
            buf = np.frombuffer(fig.canvas.buffer_rgba(), dtype=np.uint8)
            buf = buf.reshape(fig.canvas.get_width_height()[::-1] + (4,))
            # è½¬æ¢ä¸ºRGB
            buf = buf[:, :, :3]
        except AttributeError:
            try:
                # æ—§ç‰ˆæœ¬matplotlib
                buf = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)
                buf = buf.reshape(fig.canvas.get_width_height()[::-1] + (3,))
            except AttributeError:
                # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨savefigåˆ°å†…å­˜
                import io
                buf = io.BytesIO()
                fig.savefig(buf, format='png', dpi=self.dpi, bbox_inches='tight', 
                           facecolor='white', edgecolor='none')
                buf.seek(0)
                from PIL import Image
                img = Image.open(buf)
                buf = np.array(img)
                if buf.shape[2] == 4:  # RGBA to RGB
                    buf = buf[:, :, :3]
        
        plt.close(fig)
        
        return buf


class DataAugmentation:
    """æ•°æ®å¢å¼ºç±»"""
    
    def __init__(self):
        self.transforms = transforms.Compose([
            transforms.ToPILImage(),
            transforms.RandomRotation(degrees=5),
            transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),
            transforms.ColorJitter(brightness=0.1, contrast=0.1),
            transforms.ToTensor(),
        ])
    
    def augment_image(self, image):
        """
        å¯¹å›¾åƒè¿›è¡Œå¢å¼ºï¼ˆå¢å¼ºç‰ˆï¼šæ›´å¤šé¢œè‰²å˜åŒ–ï¼‰
        
        Args:
            image: PIL Image å¯¹è±¡æˆ– numpy æ•°ç»„
        
        Returns:
            PIL Image å¯¹è±¡ï¼ˆå¢å¼ºåï¼‰
        """
        # å¦‚æœè¾“å…¥æ˜¯numpyæ•°ç»„ï¼Œè½¬æ¢ä¸ºPIL Image
        if isinstance(image, np.ndarray):
            pil_image = Image.fromarray(image)
        elif isinstance(image, Image.Image):
            pil_image = image
        else:
            raise TypeError(f"Unsupported image type: {type(image)}")
        
        enhanced = pil_image
        
        # 1. äº®åº¦å¢å¼ºï¼ˆèŒƒå›´æ›´å¤§ï¼š0.7-1.3ï¼‰
        if np.random.random() < 0.8:  # 80%æ¦‚ç‡åº”ç”¨
            brightness_factor = np.random.uniform(0.7, 1.3)
            enhanced = ImageEnhance.Brightness(enhanced).enhance(brightness_factor)
        
        # 2. å¯¹æ¯”åº¦å¢å¼ºï¼ˆèŒƒå›´æ›´å¤§ï¼š0.7-1.3ï¼‰
        if np.random.random() < 0.8:  # 80%æ¦‚ç‡åº”ç”¨
            contrast_factor = np.random.uniform(0.7, 1.3)
            enhanced = ImageEnhance.Contrast(enhanced).enhance(contrast_factor)
        
        # 3. é¥±å’Œåº¦å¢å¼ºï¼ˆæ–°å¢ï¼šæ”¹å˜é¢œè‰²é²œè‰³ç¨‹åº¦ï¼‰
        if np.random.random() < 0.7:  # 70%æ¦‚ç‡åº”ç”¨
            saturation_factor = np.random.uniform(0.5, 1.5)
            enhanced = ImageEnhance.Color(enhanced).enhance(saturation_factor)
        
        # 4. è‰²å½©å¢å¼ºï¼ˆæ–°å¢ï¼šæ•´ä½“è‰²è°ƒåç§»ï¼‰
        if np.random.random() < 0.6:  # 60%æ¦‚ç‡åº”ç”¨
            color_factor = np.random.uniform(0.8, 1.2)
            enhanced = ImageEnhance.Color(enhanced).enhance(color_factor)
        
        # 5. é”åº¦å¢å¼ºï¼ˆæ–°å¢ï¼šè®©çº¿æ¡æ›´æ¸…æ™°æˆ–æ›´æ¨¡ç³Šï¼‰
        if np.random.random() < 0.5:  # 50%æ¦‚ç‡åº”ç”¨
            sharpness_factor = np.random.uniform(0.5, 1.5)
            enhanced = ImageEnhance.Sharpness(enhanced).enhance(sharpness_factor)
        
        # 6. éšæœºæ—‹è½¬ï¼ˆè§’åº¦èŒƒå›´æ›´å¤§ï¼š-5åˆ°+5åº¦ï¼‰
        if np.random.random() < 0.6:  # 60%æ¦‚ç‡åº”ç”¨
            angle = np.random.uniform(-5, 5)
            enhanced = enhanced.rotate(angle, fillcolor='white', expand=False)
        
        # 7. éšæœºæ°´å¹³ç¿»è½¬ï¼ˆæ–°å¢ï¼šé•œåƒç¿»è½¬ï¼‰
        if np.random.random() < 0.3:  # 30%æ¦‚ç‡åº”ç”¨
            enhanced = ImageOps.mirror(enhanced)
        
        # è¿”å›PIL Imageï¼ˆä¸æ˜¯tensorï¼Œå› ä¸ºpair_datasetéœ€è¦PIL Imageï¼‰
        return enhanced


class CLIPEncoder(nn.Module):
    """åŸºäºCLIPçš„ç¼–ç å™¨"""
    
    def __init__(self, model_name: str = "ViT-B/32", embedding_dim: int = 512):
        super().__init__()
        
        if not CLIP_AVAILABLE:
            raise ImportError("CLIP is not available. Please install it first.")
        
        # åŠ è½½é¢„è®­ç»ƒçš„CLIPæ¨¡å‹
        self.clip_model, self.preprocess = clip.load(model_name, device="cpu")
        
        # å†»ç»“CLIPå‚æ•°
        for param in self.clip_model.parameters():
            param.requires_grad = False
        
        # è·å–CLIPçš„è§†è§‰ç¼–ç å™¨è¾“å‡ºç»´åº¦
        clip_dim = self.clip_model.visual.output_dim
        
        # æŠ•å½±å±‚
        self.projection = nn.Sequential(
            nn.Linear(clip_dim, embedding_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(embedding_dim, embedding_dim)
        )
        
        self.embedding_dim = embedding_dim
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        # ä½¿ç”¨CLIPçš„è§†è§‰ç¼–ç å™¨
        with torch.no_grad():
            clip_features = self.clip_model.encode_image(x)
        
        # æŠ•å½±åˆ°ç›®æ ‡ç»´åº¦
        embedding = self.projection(clip_features)
        
        # L2å½’ä¸€åŒ–
        embedding = torch.nn.functional.normalize(embedding, p=2, dim=1)
        
        return embedding


class KLineDataset(Dataset):
    """Kçº¿å›¾æ•°æ®é›†"""
    
    def __init__(
        self,
        data_file: str,
        start_year: int,
        end_year: int,
        window_size: int = 5,
        step_size: int = 3,
        image_size: int = 224,
        mode: str = "train"
    ):
        self.data_file = Path(data_file)
        self.start_year = start_year
        self.end_year = end_year
        self.window_size = window_size
        self.step_size = step_size
        self.image_size = image_size
        self.mode = mode
        
        # åˆå§‹åŒ–æ¸²æŸ“å™¨å’Œå¢å¼ºå™¨
        self.renderer = CandlestickRenderer(image_size=image_size)
        self.augmenter = DataAugmentation()
        
        # åŠ è½½æ•°æ®
        self._load_data()
    
    def _load_data(self):
        """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
        print(f"Loading {self.mode} data from {self.data_file}...")
        
        # è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv(self.data_file)
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        
        # æŒ‰å¹´ä»½è¿‡æ»¤
        df_filtered = df[df['timestamp'].dt.year.between(self.start_year, self.end_year)]
        
        if len(df_filtered) == 0:
            raise ValueError(f"No data found for years {self.start_year}-{self.end_year}")
        
        print(f"Filtered data: {len(df_filtered)} records")
        print(f"Date range: {df_filtered['timestamp'].min().date()} to {df_filtered['timestamp'].max().date()}")
        
        # ç”Ÿæˆçª—å£
        self.windows = []
        for i in range(0, len(df_filtered) - self.window_size + 1, self.step_size):
            window_data = df_filtered.iloc[i:i + self.window_size]
            if len(window_data) == self.window_size:
                self.windows.append(window_data)
        
        print(f"Generated {len(self.windows)} windows")
    
    def __len__(self):
        return len(self.windows)
    
    def __getitem__(self, idx):
        """è·å–æ•°æ®é¡¹"""
        window = self.windows[idx]
        
        # æ¸²æŸ“èœ¡çƒ›å›¾
        image = self.renderer.render_candlestick(window)
        
        # æ•°æ®å¢å¼ºç”Ÿæˆæ­£æ ·æœ¬å¯¹
        anchor_image = self.augmenter.augment_image(image)
        positive_image = self.augmenter.augment_image(image)
        
        # æå–çª—å£çš„åŸºæœ¬ä¿¡æ¯ï¼ˆé¿å…è¿”å›DataFrameï¼‰
        window_info = {
            'start_date': window['timestamp'].iloc[0].strftime('%Y-%m-%d'),
            'end_date': window['timestamp'].iloc[-1].strftime('%Y-%m-%d'),
            'start_price': float(window['open'].iloc[0]),
            'end_price': float(window['close'].iloc[-1]),
            'price_change': float((window['close'].iloc[-1] - window['open'].iloc[0]) / window['open'].iloc[0])
        }
        
        return {
            "anchor": anchor_image,
            "positive": positive_image,
            "window_info": window_info,
            "index": idx
        }


class ContrastiveLoss(nn.Module):
    """å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°"""
    
    def __init__(self, temperature: float = 0.07):
        super().__init__()
        self.temperature = temperature
    
    def forward(self, anchor: torch.Tensor, positive: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—InfoNCEæŸå¤±
        
        Args:
            anchor: é”šç‚¹åµŒå…¥ [batch_size, embedding_dim]
            positive: æ­£æ ·æœ¬åµŒå…¥ [batch_size, embedding_dim]
        """
        batch_size = anchor.shape[0]
        device = anchor.device
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        anchor_norm = torch.nn.functional.normalize(anchor, p=2, dim=1)
        positive_norm = torch.nn.functional.normalize(positive, p=2, dim=1)
        
        # è®¡ç®—æ­£æ ·æœ¬ç›¸ä¼¼åº¦
        pos_sim = torch.sum(anchor_norm * positive_norm, dim=1) / self.temperature
        
        # è®¡ç®—æ‰€æœ‰æ ·æœ¬é—´çš„ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆç”¨äºè´Ÿæ ·æœ¬ï¼‰
        all_embeddings = torch.cat([anchor_norm, positive_norm], dim=0)
        sim_matrix = torch.mm(all_embeddings, all_embeddings.t()) / self.temperature
        
        # åˆ›å»ºæ©ç ï¼Œæ’é™¤å¯¹è§’çº¿
        mask = torch.eye(batch_size, device=device).bool()
        mask = torch.cat([mask, mask], dim=0)
        mask = torch.cat([mask, mask], dim=1)
        
        # è·å–è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦
        neg_sim = sim_matrix[mask].view(batch_size, -1)
        
        # è®¡ç®—logits
        logits = torch.cat([pos_sim.unsqueeze(1), neg_sim], dim=1)
        
        # åˆ›å»ºæ ‡ç­¾ï¼ˆæ­£æ ·æœ¬åœ¨ä½ç½®0ï¼‰
        labels = torch.zeros(batch_size, dtype=torch.long, device=device)
        
        # è®¡ç®—äº¤å‰ç†µæŸå¤±
        loss = torch.nn.functional.cross_entropy(logits, labels)
        
        return loss


class CLIPTrainer:
    """CLIPå¯¹æ¯”å­¦ä¹ è®­ç»ƒå™¨"""
    
    def __init__(
        self,
        model: nn.Module,
        loss_fn: nn.Module,
        optimizer: optim.Optimizer,
        device: torch.device,
        log_dir: str = None  # å¦‚æœä¸ºNoneï¼Œå°†åœ¨ä½¿ç”¨æ—¶åŸºäºè„šæœ¬ä½ç½®è®¡ç®—
    ):
        self.model = model
        self.loss_fn = loss_fn
        self.optimizer = optimizer
        self.device = device
        
        # å¦‚æœlog_dirä¸ºNoneï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„ï¼ˆåŸºäºè„šæœ¬ä½ç½®ï¼‰
        if log_dir is None:
            script_dir = Path(__file__).parent
            project_root = script_dir.parent.parent
            log_dir = str(project_root / "services" / "training" / "logs" / "clip_contrastive")
        
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
        self.model.to(device)
        
        # åˆå§‹åŒ–TensorBoard
        self.writer = SummaryWriter(log_dir=str(self.log_dir))
        
        # è®­ç»ƒçŠ¶æ€
        self.epoch = 0
        self.global_step = 0
        self.best_loss = float('inf')
    
    def load_checkpoint(self, checkpoint_path: str):
        """ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ"""
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        # åŠ è½½æ¨¡å‹çŠ¶æ€
        self.model.load_state_dict(checkpoint['model_state_dict'])
        
        # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        # æ¢å¤è®­ç»ƒçŠ¶æ€
        self.epoch = checkpoint.get('epoch', 0) + 1  # ä»ä¸‹ä¸€ä¸ªepochå¼€å§‹
        self.global_step = checkpoint.get('global_step', 0)
        self.best_loss = checkpoint.get('best_loss', float('inf'))
        
        print(f"âœ… Loaded checkpoint from epoch {checkpoint.get('epoch', 0)}")
        print(f"   Best loss so far: {self.best_loss:.4f}")
        print(f"   Will resume from epoch {self.epoch}")
    
    def train_epoch(self, dataloader: DataLoader, epoch: int) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        total_loss = 0.0
        num_batches = 0
        
        pbar = tqdm(dataloader, desc=f"Epoch {epoch}")
        
        for batch_idx, batch in enumerate(pbar):
            anchor_images = batch["anchor"].to(self.device)
            positive_images = batch["positive"].to(self.device)
            
            # å‰å‘ä¼ æ’­
            anchor_embeddings = self.model(anchor_images)
            positive_embeddings = self.model(positive_images)
            
            # è®¡ç®—æŸå¤±
            loss = self.loss_fn(anchor_embeddings, positive_embeddings)
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
            
            # è®°å½•
            total_loss += loss.item()
            num_batches += 1
            self.global_step += 1
            
            # æ¯ 10 ä¸ª step åˆ·æ–°ä¸€æ¬¡æ—¥å¿—ï¼ˆé¿å…ä¸¢å¤±æ•°æ®ï¼‰
            if self.global_step % 10 == 0:
                self.writer.flush()
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                "loss": f"{loss.item():.4f}",
                "avg_loss": f"{total_loss/num_batches:.4f}"
            })
            
            # è®°å½•åˆ°TensorBoard
            if self.global_step % 10 == 0:
                self.writer.add_scalar("train/loss", loss.item(), self.global_step)
                self.writer.add_scalar("train/learning_rate", 
                                     self.optimizer.param_groups[0]['lr'], 
                                     self.global_step)
        
        avg_loss = total_loss / num_batches
        return {"loss": avg_loss}
    
    def validate(self, dataloader: DataLoader) -> Dict[str, float]:
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        
        total_loss = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for batch in tqdm(dataloader, desc="Validation"):
                anchor_images = batch["anchor"].to(self.device)
                positive_images = batch["positive"].to(self.device)
                
                # å‰å‘ä¼ æ’­
                anchor_embeddings = self.model(anchor_images)
                positive_embeddings = self.model(positive_images)
                
                # è®¡ç®—æŸå¤±
                loss = self.loss_fn(anchor_embeddings, positive_embeddings)
                
                total_loss += loss.item()
                num_batches += 1
        
        if num_batches == 0:
            return {"loss": float("inf")}  # æ— éªŒè¯æ ·æœ¬æ—¶é¿å…é™¤é›¶
        avg_loss = total_loss / num_batches
        return {"loss": avg_loss}
    
    def save_checkpoint(self, epoch: int, metrics: Dict[str, float], is_best: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            "epoch": epoch,
            "model_state_dict": self.model.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "metrics": metrics,
            "global_step": self.global_step,
            "best_loss": self.best_loss
        }
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        checkpoint_path = self.log_dir / "checkpoint_latest.pth"
        torch.save(checkpoint, checkpoint_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = self.log_dir / "checkpoint_best.pth"
            torch.save(checkpoint, best_path)
            print(f"âœ… Saved best model at epoch {epoch}")
    
    def train(self, train_loader: DataLoader, val_loader: DataLoader, num_epochs: int):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f"ğŸš€ Starting CLIP contrastive learning for {num_epochs} epochs...")
        
        for epoch in range(self.epoch, num_epochs):
            print(f"\nğŸ“Š Epoch {epoch+1}/{num_epochs}")
            
            # è®­ç»ƒ
            train_metrics = self.train_epoch(train_loader, epoch)
            
            # æœ‰éªŒè¯é›†æ‰åšéªŒè¯ï¼›æ— éªŒè¯é›†æ—¶ä»…ç”¨è®­ç»ƒæŸå¤±åšæœ€ä½³åˆ¤å®šä¸ä¿å­˜
            has_val = len(val_loader.dataset) > 0
            if has_val:
                val_metrics = self.validate(val_loader)
                self.writer.add_scalar("epoch/val_loss", val_metrics["loss"], epoch)
                is_best = val_metrics["loss"] < self.best_loss
                if is_best:
                    self.best_loss = val_metrics["loss"]
                save_metrics = val_metrics
            else:
                is_best = train_metrics["loss"] < self.best_loss
                if is_best:
                    self.best_loss = train_metrics["loss"]
                save_metrics = train_metrics
            
            self.writer.add_scalar("epoch/train_loss", train_metrics["loss"], epoch)
            self.writer.flush()
            
            if (epoch + 1) % 5 == 0 or is_best:
                self.save_checkpoint(epoch, save_metrics, is_best)
            
            print(f"Train Loss: {train_metrics['loss']:.4f}", end="")
            if has_val:
                print(f"  |  Val Loss: {val_metrics['loss']:.4f}", end="")
            if is_best:
                print("  |  ğŸ‰ New best model!", end="")
            print()
        
        print("âœ… Training completed!")
        # ç¡®ä¿æ‰€æœ‰æ—¥å¿—éƒ½å†™å…¥ç£ç›˜
        self.writer.flush()
        # ç­‰å¾…ä¸€ä¸‹ç¡®ä¿æ•°æ®å®Œå…¨å†™å…¥
        import time
        time.sleep(0.5)
        self.writer.close()
        # å†æ¬¡ç­‰å¾…ç¡®ä¿æ–‡ä»¶å…³é—­
        time.sleep(0.5)
        print(f"ğŸ“Š TensorBoard logs saved to: {self.log_dir}")
        print(f"   Events files: {list(self.log_dir.glob('*.tfevents*'))}")


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆåŸºäºè„šæœ¬ä½ç½®ï¼‰
    script_dir = Path(__file__).parent
    project_root = script_dir.parent.parent
    
    parser = argparse.ArgumentParser(description="Train CLIP-based K-line contrastive encoder")
    
    # æ•°æ®å‚æ•°ï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„ä½œä¸ºé»˜è®¤å€¼ï¼‰
    default_data_file = str(project_root / "services" / "training" / "data" / "dow30_real_AAPL.csv")
    parser.add_argument("--data_file", type=str, default=default_data_file)
    parser.add_argument("--train_start_year", type=int, default=2012)
    parser.add_argument("--train_end_year", type=int, default=2016)
    parser.add_argument("--test_year", type=int, default=2017)
    parser.add_argument("--window_size", type=int, default=5)
    parser.add_argument("--step_size", type=int, default=3)
    parser.add_argument("--image_size", type=int, default=224)
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--clip_model", type=str, default="ViT-B/32")
    parser.add_argument("--embedding_dim", type=int, default=512)
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--batch_size", type=int, default=16)
    parser.add_argument("--num_epochs", type=int, default=50)
    parser.add_argument("--learning_rate", type=float, default=1e-4)
    parser.add_argument("--weight_decay", type=float, default=1e-5)
    
    # æŸå¤±å‡½æ•°å‚æ•°
    parser.add_argument("--temperature", type=float, default=0.07)
    
    # å…¶ä»–å‚æ•°ï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„ä½œä¸ºé»˜è®¤å€¼ï¼‰
    default_log_dir = str(project_root / "services" / "training" / "logs" / "clip_contrastive")
    parser.add_argument("--device", type=str, default="auto")
    parser.add_argument("--log_dir", type=str, default=default_log_dir)
    
    args = parser.parse_args()
    
    # å°†ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„ï¼ˆå¦‚æœç”¨æˆ·æä¾›äº†ç›¸å¯¹è·¯å¾„ï¼‰
    if not Path(args.data_file).is_absolute():
        args.data_file = str(Path(args.data_file).resolve())
    if not Path(args.log_dir).is_absolute():
        args.log_dir = str(Path(args.log_dir).resolve())
    
    # æ£€æŸ¥CLIPæ˜¯å¦å¯ç”¨
    if not CLIP_AVAILABLE:
        print("âŒ CLIP is not available. Please install it first:")
        print("pip install git+https://github.com/openai/CLIP.git")
        return
    
    # è®¾ç½®è®¾å¤‡
    if args.device == "auto":
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    else:
        device = torch.device(args.device)
    
    print(f"ğŸ”§ Using device: {device}")
    
    # åˆ›å»ºæ•°æ®é›†
    print("ğŸ“ Loading datasets...")
    train_dataset = KLineDataset(
        data_file=args.data_file,
        start_year=args.train_start_year,
        end_year=args.train_end_year,
        window_size=args.window_size,
        step_size=args.step_size,
        image_size=args.image_size,
        mode="train"
    )
    
    test_dataset = KLineDataset(
        data_file=args.data_file,
        start_year=args.test_year,
        end_year=args.test_year,
        window_size=args.window_size,
        step_size=args.step_size,
        image_size=args.image_size,
        mode="test"
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset, 
        batch_size=args.batch_size, 
        shuffle=True,
        num_workers=2,
        pin_memory=True
    )
    n_train = len(train_dataset)
    n_batches = len(train_loader)
    print(f"ğŸ“Š è®­ç»ƒé›†: {n_train} æ¡æ ·æœ¬, æ¯ epoch {n_batches} ä¸ª batch (batch_size={args.batch_size})")
    
    test_loader = DataLoader(
        test_dataset, 
        batch_size=args.batch_size, 
        shuffle=False,
        num_workers=2,
        pin_memory=True
    )
    
    # åˆ›å»ºæ¨¡å‹
    print("ğŸ—ï¸ Creating CLIP-based model...")
    model = CLIPEncoder(
        model_name=args.clip_model,
        embedding_dim=args.embedding_dim
    )
    
    # åˆ›å»ºæŸå¤±å‡½æ•°
    loss_fn = ContrastiveLoss(temperature=args.temperature)
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = optim.AdamW(
        model.parameters(),
        lr=args.learning_rate,
        weight_decay=args.weight_decay
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = CLIPTrainer(
        model=model,
        loss_fn=loss_fn,
        optimizer=optimizer,
        device=device,
        log_dir=args.log_dir
    )
    
    # å°è¯•ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒï¼ˆé»˜è®¤è‡ªåŠ¨æ¢å¤ï¼‰
    checkpoint_path = Path(args.log_dir) / "checkpoint_latest.pth"
    if checkpoint_path.exists():
        print(f"ğŸ“ Found checkpoint: {checkpoint_path}")
        try:
            trainer.load_checkpoint(str(checkpoint_path))
        except Exception as e:
            print(f"âš ï¸  Failed to load checkpoint: {e}")
            print("   Starting training from scratch")
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train(
        train_loader=train_loader,
        val_loader=test_loader,
        num_epochs=args.num_epochs
    )


if __name__ == "__main__":
    main()
